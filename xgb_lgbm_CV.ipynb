{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1HYkzgmxyEjTO0bOkk6mSIMgHi7kuQ_TN",
      "authorship_tag": "ABX9TyNo6eEDIYGuVLex7kZSKPzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagasora/NeurIPS---Open-Polymer-Prediction-2025/blob/main/xgb_lgbm_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "w2Z2GK4pcr63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "neurips_open_polymer_prediction_2025_path = kagglehub.competition_download('neurips-open-polymer-prediction-2025')\n",
        "senkin13_rdkit_2025_3_3_cp311_path = kagglehub.dataset_download('senkin13/rdkit-2025-3-3-cp311')\n",
        "minatoyukinaxlisa_tc_smiles_path = kagglehub.dataset_download('minatoyukinaxlisa/tc-smiles')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "T9-NWHOpgGXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'neurips_open_polymer_prediction_2025_path: {neurips_open_polymer_prediction_2025_path}')"
      ],
      "metadata": {
        "id": "Ta8Pz3NdgZG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWS7xPzPai80"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# ライブラリのインストール (Kaggle/Colab環境で最初に実行)\n",
        "# ===================================================================\n",
        "!pip install rdkit --quiet\n",
        "!pip install xgboost --quiet\n",
        "!pip install optuna --quiet\n",
        "!pip install tqdm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# ライブラリのインポート\n",
        "# ===================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, MACCSkeys\n",
        "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
        "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from tqdm.auto import tqdm\n",
        "import optuna\n",
        "import joblib\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# ログ設定\n",
        "# ===================================================================\n",
        "# すべての警告を抑制\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2ID1DCzDatFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# ログ設定\n",
        "# ===================================================================\n",
        "# すべての警告を抑制\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 詳細なフォーマットでロギングを設定\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('polymer_prediction_xgb.log', mode='w')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Optunaの冗長な出力を抑制\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n"
      ],
      "metadata": {
        "id": "6dO2ss1HdLe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 3. 設定クラスと基本設定\n",
        "# ===================================================================\n",
        "class CFG:\n",
        "    seed = 42\n",
        "    n_splits = 5\n",
        "    data_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/'\n",
        "    supp_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/train_supplement/'\n",
        "    output_dir = '/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+XGB+GNN_CV/'\n",
        "    lgbm_model_dir = os.path.join(output_dir, 'models_lgbm')\n",
        "    xgb_model_dir = os.path.join(output_dir, 'models_xgb')\n",
        "    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['LIGHTGBM_VERBOSITY'] = '-1'\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "seed_everything(CFG.seed)\n",
        "\n",
        "def load_data(cfg: CFG) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    メインの学習データと追加データを読み込み、統合する関数。\n",
        "\n",
        "    Args:\n",
        "        cfg (CFG): データパスを含む設定オブジェクト\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: 全ての学習データを結合したデータフレーム\n",
        "    \"\"\"\n",
        "    logger.info(\"データの読み込みを開始します...\")\n",
        "\n",
        "    # 1. メインの学習データを読み込み\n",
        "    main_train_path = os.path.join(cfg.data_dir, 'train.csv')\n",
        "    try:\n",
        "        train_df = pd.read_csv(main_train_path)\n",
        "        logger.info(f\"メイン学習データを読み込みました。サンプル数: {len(train_df)}\")\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"メイン学習データが見つかりません: {main_train_path}\")\n",
        "        # メインデータがない場合は空のDataFrameを返して終了\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. 追加データの読み込みと整形\n",
        "    try:\n",
        "        # dataset1: Tc (熱伝導率) データ\n",
        "        supp1_path = os.path.join(cfg.supp_dir, 'dataset1.csv')\n",
        "        supp1 = pd.read_csv(supp1_path)[['SMILES', 'TC_mean']].rename(columns={'TC_mean': 'Tc'})\n",
        "\n",
        "        # dataset3: Tg (ガラス転移温度) データ\n",
        "        supp3_path = os.path.join(cfg.supp_dir, 'dataset3.csv')\n",
        "        supp3 = pd.read_csv(supp3_path)[['SMILES', 'Tg']]\n",
        "\n",
        "        # dataset4: FFV (自由体積分率) データ\n",
        "        supp4_path = os.path.join(cfg.supp_dir, 'dataset4.csv')\n",
        "        supp4 = pd.read_csv(supp4_path)[['SMILES', 'FFV']]\n",
        "\n",
        "        # 3. 全てのデータフレームを結合\n",
        "        train_df = pd.concat([train_df, supp1, supp3, supp4], ignore_index=True)\n",
        "\n",
        "        # 重複するSMILESがあれば削除 (任意ですが、クリーニングとして推奨)\n",
        "        train_df = train_df.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n",
        "\n",
        "        logger.info(f\"✅ 追加データを統合しました。総学習サンプル数: {len(train_df)}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        logger.warning(f\"⚠️ 追加データが見つかりませんでした: {e}\")\n",
        "        logger.warning(\"メインデータのみで学習を続行します。\")\n",
        "\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "aR7To2vXdMPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ 修正箇所 ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼\n",
        "# 4. チューニング済みハイパーパラメータの定義\n",
        "# ===================================================================\n",
        "# xgb_finetune.ipynb から得られた最適なハイパーパラメータをここに記述します\n",
        "# (これはサンプル値です。実際のチューニング結果に合わせて値を変更してください)\n",
        "XGB_BEST_PARAMS = joblib.load(\"/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+GNN/xgb_tuned/best_params_xgb.joblib\")\n",
        "\n",
        "# LGBMは固定の高性能パラメータを使用\n",
        "LGBM_PARAMS = joblib.load('/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+GNN/lgbm_tuned/best_params.joblib')\n",
        "# ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲ 修正箇所 ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
        "\n",
        "# ===================================================================\n",
        "# 5. データ読み込みと特徴量抽出\n",
        "# ===================================================================\n",
        "# 特徴量抽出関数 (前回と同じコードのため省略)\n",
        "# --- モデルごとに特徴量抽出関数を定義 ---\n",
        "\n",
        "def extract_lgbm_features(smiles_list: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    LGBMモデルの訓練時と同一の特徴量を生成する。\n",
        "    (MorganFP + MACCS Keys + All Descriptors)\n",
        "    \"\"\"\n",
        "    logger.info(\"LGBM用の特徴量を抽出中...\")\n",
        "    descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
        "    desc_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
        "    morgan_gen = GetMorganGenerator(radius=2, fpSize=1024)\n",
        "\n",
        "    features = []\n",
        "    num_features = 1024 + 167 + len(descriptor_names)\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                features.append(np.zeros(num_features))\n",
        "                continue\n",
        "\n",
        "            # フィンガープリント計算 (Morgan + MACCS)\n",
        "            morgan_fp = np.array(morgan_gen.GetFingerprintAsNumPy(mol))\n",
        "            maccs_fp = np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "            fp_features = np.concatenate([morgan_fp, maccs_fp])\n",
        "\n",
        "            # 記述子計算\n",
        "            desc_features = np.array(desc_calculator.CalcDescriptors(mol))\n",
        "\n",
        "            all_features = np.concatenate([fp_features, desc_features])\n",
        "            features.append(all_features)\n",
        "        except Exception:\n",
        "            features.append(np.zeros(num_features))\n",
        "\n",
        "    feature_matrix = np.array(features)\n",
        "    return np.nan_to_num(feature_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "def extract_xgb_features(smiles_list: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    XGBoostモデルの訓練時と同一の特徴量を生成する。\n",
        "    (MorganFP + All Descriptors)\n",
        "    \"\"\"\n",
        "    logger.info(\"XGBoost用の特徴量を抽出中...\")\n",
        "    descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
        "    desc_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
        "    morgan_gen = GetMorganGenerator(radius=2, fpSize=1024)\n",
        "\n",
        "    features = []\n",
        "    num_features = 1024 + len(descriptor_names)\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is None:\n",
        "                features.append(np.zeros(num_features))\n",
        "                continue\n",
        "\n",
        "            # フィンガープリント計算 (Morganのみ)\n",
        "            fp = np.array(morgan_gen.GetFingerprintAsNumPy(mol))\n",
        "\n",
        "            # 記述子計算\n",
        "            descs = np.array(desc_calculator.CalcDescriptors(mol))\n",
        "\n",
        "            all_features = np.concatenate([fp, descs])\n",
        "            features.append(all_features)\n",
        "        except Exception:\n",
        "            features.append(np.zeros(num_features))\n",
        "\n",
        "    feature_matrix = np.array(features)\n",
        "    return np.nan_to_num(feature_matrix, nan=0.0, posinf=0.0, neginf=0.0)\n"
      ],
      "metadata": {
        "id": "KkDF7oa_Qqi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 6. CV学習・推論クラス\n",
        "# ===================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "class PolymerCVTrainer:\n",
        "    def __init__(self, model_type: str, base_params: Dict[str, Any], model_save_dir: str, seed: int = 42):\n",
        "        if model_type not in ['lgbm', 'xgb']:\n",
        "            raise ValueError(\"model_typeは 'lgbm' または 'xgb' である必要があります\")\n",
        "        self.model_type = model_type\n",
        "        self.base_params = base_params\n",
        "        # ▼▼▼ 修正箇所 ▼▼▼\n",
        "        # モデル保存用のディレクトリを初期化時に受け取る\n",
        "        self.model_save_dir = model_save_dir\n",
        "        os.makedirs(self.model_save_dir, exist_ok=True) # ディレクトリ作成\n",
        "        # ▲▲▲ 修正箇所 ▲▲▲\n",
        "        self.seed = seed\n",
        "\n",
        "    def train_predict(self, train_df: pd.DataFrame, test_df: pd.DataFrame, targets: List[str], feature_extractor_func) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        X = feature_extractor_func(train_df['SMILES'].tolist())\n",
        "        X_test = feature_extractor_func(test_df['SMILES'].tolist())\n",
        "\n",
        "        oof_preds = pd.DataFrame(index=train_df.index, columns=targets, dtype=np.float32)\n",
        "        test_preds_all_folds = np.zeros((len(test_df), len(targets), CFG.n_splits))\n",
        "\n",
        "        kf = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=self.seed)\n",
        "\n",
        "        for i, target in enumerate(targets):\n",
        "            logger.info(f\"===== Processing Target: {target} =====\")\n",
        "\n",
        "            y = train_df[target]\n",
        "            valid_indices = y.notna()\n",
        "            X_target = X[valid_indices]\n",
        "            y_target = y[valid_indices]\n",
        "\n",
        "            # ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼ 修正箇所 ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼\n",
        "            # モデルごとに、またターゲットごとにパラメータを設定\n",
        "            model_params = self.base_params.copy()\n",
        "            if self.model_type == 'xgb':\n",
        "                # XGBoostの場合、ターゲット固有の最適パラメータで上書き\n",
        "                model_params = XGB_BEST_PARAMS[target]\n",
        "                model_params['random_state'] = self.seed\n",
        "                model_params['objective'] = 'reg:absoluteerror'\n",
        "                model_params['n_estimators'] = 2000 # 固定\n",
        "            # ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲ 修正箇所 ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲\n",
        "\n",
        "            for fold, (train_idx, val_idx) in enumerate(kf.split(X_target, y_target)):\n",
        "                logger.info(f\"--- Fold {fold+1}/{CFG.n_splits} ---\")\n",
        "                X_train, X_val = X_target[train_idx], X_target[val_idx]\n",
        "                y_train, y_val = y_target.iloc[train_idx], y_target.iloc[val_idx]\n",
        "\n",
        "                scaler = StandardScaler()\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_val_scaled = scaler.transform(X_val)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                if self.model_type == 'lgbm':\n",
        "                    model_params = LGBM_PARAMS[target]\n",
        "                    model = lgb.LGBMRegressor(**model_params)\n",
        "                    model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "                else:\n",
        "                    model = xgb.XGBRegressor(**model_params, early_stopping_rounds=50)\n",
        "                    model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)\n",
        "\n",
        "                # モデルの保存\n",
        "                    # ▼▼▼ 以下の2行を追加 ▼▼▼\n",
        "                # スケーラーの保存\n",
        "                scaler_path = os.path.join(self.model_save_dir, f\"{self.model_type}_{target}_fold_{fold}_scaler.joblib\")\n",
        "                joblib.dump(scaler, scaler_path)\n",
        "                # ▲▲▲ 追加はここまで ▲▲▲\n",
        "                model_path = os.path.join(self.model_save_dir, f\"{self.model_type}_{target}_fold_{fold}.joblib\")\n",
        "                joblib.dump(model, model_path)\n",
        "                logger.info(f\"Model saved to: {model_path}\")\n",
        "                # ▲▲▲ 修正箇所 ▲▲▲\n",
        "                val_preds = model.predict(X_val_scaled)\n",
        "                oof_preds.loc[y_target.index[val_idx], target] = val_preds\n",
        "                test_preds_all_folds[:, i, fold] = model.predict(X_test_scaled)\n",
        "\n",
        "        final_test_preds = pd.DataFrame(test_preds_all_folds.mean(axis=2), columns=targets)\n",
        "\n",
        "        for target in targets:\n",
        "            score = mean_absolute_error(train_df[target].dropna(), oof_preds[target].dropna())\n",
        "            logger.info(f\"Local CV Score for {target}: {score:.4f}\")\n",
        "\n",
        "        return oof_preds, final_test_preds\n"
      ],
      "metadata": {
        "id": "W5AgJRVRUDy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_df = load_data(CFG)\n",
        "test_df = pd.read_csv(os.path.join(CFG.data_dir, 'test.csv'))\n",
        "\n",
        "# --- LGBMのCV学習 ---\n",
        "logger.info(\"========== Starting LGBM CV Training ==========\")\n",
        "lgbm_trainer = PolymerCVTrainer(model_type='lgbm', base_params=LGBM_PARAMS, model_save_dir=CFG.lgbm_model_dir, seed=CFG.seed)\n",
        "# (LGBMの学習は時間がかかる場合コメントアウト)\n",
        "oof_preds_lgbm, test_preds_lgbm = lgbm_trainer.train_predict(full_train_df, test_df, CFG.targets, extract_lgbm_features)"
      ],
      "metadata": {
        "id": "1uaymOZJdd7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- XGBoostのCV学習 (ファインチューニング済みパラメータ使用) ---\n",
        "logger.info(\"========== Starting XGBoost CV Training with Fine-Tuned Params ==========\")\n",
        "xgb_trainer = PolymerCVTrainer(model_type='xgb', base_params={}, model_save_dir=CFG.xgb_model_dir, seed=CFG.seed) # base_paramsは中で設定するので空でOK\n",
        "oof_preds_xgb, test_preds_xgb = xgb_trainer.train_predict(full_train_df, test_df, CFG.targets, extract_xgb_features)\n"
      ],
      "metadata": {
        "id": "PRTylxwray6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 結果の保存 ---\n",
        "oof_preds_lgbm.to_csv(os.path.join(CFG.output_dir, 'oof_preds_lgbm.csv'))\n",
        "test_preds_lgbm.to_csv(os.path.join(CFG.output_dir, 'test_preds_lgbm.csv'))\n",
        "oof_preds_xgb.to_csv(os.path.join(CFG.output_dir, 'oof_preds_xgb.csv'), index=False)\n",
        "test_preds_xgb.to_csv(os.path.join(CFG.output_dir, 'test_preds_xgb.csv'), index=False)\n",
        "logger.info(\"OOF予測とテスト予測をファイルに保存しました。\")"
      ],
      "metadata": {
        "id": "R4Zd-_ZXa07f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 提出ファイルの作成 (今回はXGB単体) ---\n",
        "submission_df = pd.DataFrame({'id': test_df['id']})\n",
        "for target in CFG.targets:\n",
        "    submission_df[target] = test_preds_xgb[target]\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "logger.info(\"✅ 提出ファイル 'submission.csv' を作成しました。\")"
      ],
      "metadata": {
        "id": "P-nWGbi2a1lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.head()"
      ],
      "metadata": {
        "id": "04hD_ml_efcW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# CV実行時に保存したOOF予測ファイルを読み込む\n",
        "oof_lgbm = pd.read_csv(\"/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+XGB_CV/oof_preds_lgbm.csv\")\n",
        "oof_xgb = pd.read_csv(\"/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+XGB_CV/oof_preds_xgb.csv\")\n",
        "train_df = pd.read_csv(\"/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/train.csv\") # 正解ラベルの読み込み\n",
        "\n",
        "best_score = float('inf')\n",
        "best_weight = 0\n",
        "\n",
        "# 0.01刻みで最適な重みを探す\n",
        "for w in np.arange(0, 1.01, 0.01):\n",
        "    # 加重平均でOOF予測をブレンド\n",
        "    oof_blend = w * oof_lgbm + (1 - w) * oof_xgb\n",
        "\n",
        "    # スコアを計算 (ターゲットごとに計算し平均する)\n",
        "    scores = []\n",
        "    for target in ['Tg', 'FFV', 'Tc', 'Density', 'Rg']:\n",
        "         # NaNを削除してスコアを計算\n",
        "        y_true = train_df[target].dropna()\n",
        "        y_pred = oof_blend[target].loc[y_true.index].dropna()\n",
        "        scores.append(mean_absolute_error(y_true, y_pred))\n",
        "\n",
        "    avg_score = np.mean(scores)\n",
        "\n",
        "    if avg_score < best_score:\n",
        "        best_score = avg_score\n",
        "        best_weight = w\n",
        "\n",
        "print(f\"最適なLGBMの重み: {best_weight:.2f}\")\n",
        "print(f\"最適なXGBの重み: {1-best_weight:.2f}\")\n",
        "print(f\"予測されるベストスコア: {best_score:.5f}\")"
      ],
      "metadata": {
        "id": "iMMpWizVncFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
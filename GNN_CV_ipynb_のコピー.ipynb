{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP70uX1rSTkom15USVkAaLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagasora/NeurIPS---Open-Polymer-Prediction-2025/blob/main/GNN_CV_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwOAv4XnDtG"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "neurips_open_polymer_prediction_2025_path = kagglehub.competition_download('neurips-open-polymer-prediction-2025')\n",
        "senkin13_rdkit_2025_3_3_cp311_path = kagglehub.dataset_download('senkin13/rdkit-2025-3-3-cp311')\n",
        "minatoyukinaxlisa_tc_smiles_path = kagglehub.dataset_download('minatoyukinaxlisa/tc-smiles')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "iJsnFnwOnGa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'neurips_open_polymer_prediction_2025_path: {neurips_open_polymer_prediction_2025_path}')"
      ],
      "metadata": {
        "id": "RYMTrCf8nIP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "# ===================================================================\n",
        "!pip install rdkit --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install torch_geometric --quiet"
      ],
      "metadata": {
        "id": "1UDXoKR6nJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "# ===================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F # Import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "TMxAA7uZnOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 3. è¨­å®šã‚¯ãƒ©ã‚¹ã¨åŸºæœ¬è¨­å®š\n",
        "# ===================================================================\n",
        "class CFG:\n",
        "    seed = 42\n",
        "    n_splits = 5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    data_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/'\n",
        "    supp_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/train_supplement/'\n",
        "    output_dir = '/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+XGB+GNN_CV/'\n",
        "    lgbm_model_dir = os.path.join(output_dir, 'models_lgbm')\n",
        "    xgb_model_dir = os.path.join(output_dir, 'models_xgb')\n",
        "    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    node_feature_dim = 40\n",
        "    hidden_dim = 256\n",
        "    output_dim = 5\n",
        "    num_gnn_layers = 4\n",
        "    dropout_rate = 0.2\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    epochs = 100 # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™\n",
        "    batch_size = 64\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 1e-5\n",
        "    patience = 10 # Early Stoppingã®ãŸã‚ã®patience\n",
        "\n",
        "    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "seed_everything(CFG.seed)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "em8LfuefnRhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_IGYGjhn7wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(cfg: CFG) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€çµ±åˆã™ã‚‹é–¢æ•°ã€‚\n",
        "\n",
        "    Args:\n",
        "        cfg (CFG): ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’å«ã‚€è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: å…¨ã¦ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
        "    \"\"\"\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "    # 1. ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
        "    main_train_path = os.path.join(cfg.data_dir, 'train.csv')\n",
        "    try:\n",
        "        train_df = pd.read_csv(main_train_path)\n",
        "        print(f\"ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_df)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {main_train_path}\")\n",
        "        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã—ã¦çµ‚äº†\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ•´å½¢\n",
        "    try:\n",
        "        # dataset1: Tc (ç†±ä¼å°ç‡) ãƒ‡ãƒ¼ã‚¿\n",
        "        supp1_path = os.path.join(cfg.supp_dir, 'dataset1.csv')\n",
        "        supp1 = pd.read_csv(supp1_path)[['SMILES', 'TC_mean']].rename(columns={'TC_mean': 'Tc'})\n",
        "\n",
        "        # dataset3: Tg (ã‚¬ãƒ©ã‚¹è»¢ç§»æ¸©åº¦) ãƒ‡ãƒ¼ã‚¿\n",
        "        supp3_path = os.path.join(cfg.supp_dir, 'dataset3.csv')\n",
        "        supp3 = pd.read_csv(supp3_path)[['SMILES', 'Tg']]\n",
        "\n",
        "        # dataset4: FFV (è‡ªç”±ä½“ç©åˆ†ç‡) ãƒ‡ãƒ¼ã‚¿\n",
        "        supp4_path = os.path.join(cfg.supp_dir, 'dataset4.csv')\n",
        "        supp4 = pd.read_csv(supp4_path)[['SMILES', 'FFV']]\n",
        "\n",
        "        # 3. å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ\n",
        "        train_df = pd.concat([train_df, supp1, supp3, supp4], ignore_index=True)\n",
        "\n",
        "        # é‡è¤‡ã™ã‚‹SMILESãŒã‚ã‚Œã°å‰Šé™¤ (ä»»æ„ã§ã™ãŒã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã—ã¦æ¨å¥¨)\n",
        "        train_df = train_df.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n",
        "\n",
        "        print(f\"âœ… è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã—ãŸã€‚ç·å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(train_df)}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"âš ï¸ è¿½åŠ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {e}\")\n",
        "        print(\"ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§å­¦ç¿’ã‚’ç¶šè¡Œã—ã¾ã™ã€‚\")\n",
        "\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "Tkiz9cvmnby0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# SMILESã‹ã‚‰ã‚°ãƒ©ãƒ•ã¸ã®å¤‰æ›\n",
        "# ===================================================================\n",
        "def smiles_to_graph(smiles: str) -> Data:\n",
        "    \"\"\"\n",
        "    SMILESæ–‡å­—åˆ—ã‚’PyTorch Geometricã®Dataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILESæ–‡å­—åˆ—\n",
        "\n",
        "    Returns:\n",
        "        Data: ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã€ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å«ã‚€ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    # ãƒãƒ¼ãƒ‰ï¼ˆåŸå­ï¼‰ç‰¹å¾´é‡ã®æŠ½å‡º\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = [\n",
        "            atom.GetAtomicNum(),\n",
        "            atom.GetDegree(),\n",
        "            atom.GetFormalCharge(),\n",
        "            atom.GetNumRadicalElectrons(),\n",
        "            atom.GetHybridization(),\n",
        "            atom.GetIsAromatic(),\n",
        "            atom.GetTotalNumHs(),\n",
        "        ]\n",
        "        atom_features.append(feature)\n",
        "    x = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # ã‚¨ãƒƒã‚¸ï¼ˆçµåˆï¼‰æƒ…å ±ã®æŠ½å‡º\n",
        "    edge_indices = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]]) # ç„¡å‘ã‚°ãƒ©ãƒ•\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹ãŸã‚ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
        "    if x.shape[1] < CFG.node_feature_dim:\n",
        "        padding = torch.zeros(x.shape[0], CFG.node_feature_dim - x.shape[1])\n",
        "        x = torch.cat([x, padding], dim=1)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# GNNãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "# ===================================================================\n",
        "class PolymerGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ãƒãƒªãƒãƒ¼ç‰©æ€§äºˆæ¸¬ã®ãŸã‚ã®GNNãƒ¢ãƒ‡ãƒ«\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(PolymerGNN, self).__init__()\n",
        "        # GINç•³ã¿è¾¼ã¿å±¤\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GINConv(nn.Sequential(\n",
        "            nn.Linear(CFG.node_feature_dim, CFG.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "        )))\n",
        "        self.batch_norms.append(nn.BatchNorm1d(CFG.hidden_dim))\n",
        "\n",
        "        for _ in range(CFG.num_gnn_layers - 1):\n",
        "            self.convs.append(GINConv(nn.Sequential(\n",
        "                nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "            )))\n",
        "            self.batch_norms.append(nn.BatchNorm1d(CFG.hidden_dim))\n",
        "\n",
        "        # å‡ºåŠ›å±¤ï¼ˆMLPï¼‰\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(CFG.hidden_dim, CFG.hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(CFG.dropout_rate),\n",
        "            nn.Linear(CFG.hidden_dim // 2, CFG.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data: Data) -> torch.Tensor:\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.batch_norms):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "\n",
        "        # ã‚°ãƒ©ãƒ•å…¨ä½“ã®è¡¨ç¾ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®—\n",
        "        x_pooled = global_add_pool(x, batch)\n",
        "\n",
        "        # MLPã§æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’è¨ˆç®—\n",
        "        out = self.mlp(x_pooled)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Datasetã¨æå¤±é–¢æ•°ã®å†å®šç¾©ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œï¼‰\n",
        "# ===================================================================\n",
        "class PolymerDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, target_scalers: Dict[str, StandardScaler] = None):\n",
        "        self.smiles = df['SMILES'].values\n",
        "        self.labels_original = df[CFG.targets].values\n",
        "        self.target_scalers = target_scalers\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple:\n",
        "        graph = smiles_to_graph(self.smiles[idx])\n",
        "        labels = self.labels_original[idx]\n",
        "\n",
        "        if self.target_scalers:\n",
        "            # â–¼â–¼â–¼ é‡è¦ãªå¤‰æ›´ç‚¹ï¼šãƒ©ãƒ™ãƒ«ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° â–¼â–¼â–¼\n",
        "            scaled_labels = np.full_like(labels, np.nan, dtype=np.float32)\n",
        "            for i, target in enumerate(CFG.targets):\n",
        "                if not np.isnan(labels[i]):\n",
        "                    scaled_labels[i] = self.target_scalers[target].transform(labels[i].reshape(1, 1))[0, 0]\n",
        "            return graph, torch.tensor(scaled_labels, dtype=torch.float)\n",
        "        else:\n",
        "            return graph, torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# ===================================================================\n",
        "# æå¤±é–¢æ•°ã¨å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—\n",
        "# ===================================================================\n",
        "class MaskedMAELoss(nn.Module):\n",
        "    \"\"\"NaNã‚’ç„¡è¦–ã—ã¦MAEã‚’è¨ˆç®—ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MaskedMAELoss, self).__init__()\n",
        "        self.mae = nn.L1Loss(reduction='none')\n",
        "\n",
        "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "        mask = ~torch.isnan(y_true)\n",
        "        loss = self.mae(y_pred[mask], y_true[mask])\n",
        "        return loss.mean()\n",
        "\n",
        "def train_fn(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’ã‚’è¡Œã†é–¢æ•°\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def eval_fn(model, dataloader, criterion, device, target_scalers):\n",
        "    \"\"\"è©•ä¾¡é–¢æ•°ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯¾å¿œã«ä¿®æ­£\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds_inversed = []\n",
        "    all_labels_original = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, scaled_labels in dataloader:\n",
        "            data, scaled_labels = data.to(device), scaled_labels.to(device)\n",
        "\n",
        "            # ãƒ¢ãƒ‡ãƒ«ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸå€¤ã‚’äºˆæ¸¬\n",
        "            scaled_preds = model(data)\n",
        "            loss = criterion(scaled_preds, scaled_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # â–¼â–¼â–¼ é‡è¦ãªå¤‰æ›´ç‚¹ï¼šäºˆæ¸¬å€¤ã‚’å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™ â–¼â–¼â–¼\n",
        "            scaled_preds_cpu = scaled_preds.cpu().numpy()\n",
        "            inversed_preds = np.full_like(scaled_preds_cpu, np.nan)\n",
        "\n",
        "            for i, target in enumerate(CFG.targets):\n",
        "                # NaNã§ãªã„äºˆæ¸¬å€¤ã®ã¿ã‚’é€†å¤‰æ›\n",
        "                valid_mask = ~np.isnan(scaled_preds_cpu[:, i])\n",
        "                if np.any(valid_mask):\n",
        "                    inversed_preds[valid_mask, i] = target_scalers[target].inverse_transform(scaled_preds_cpu[valid_mask, i].reshape(-1, 1)).flatten()\n",
        "\n",
        "            all_preds_inversed.append(inversed_preds)\n",
        "            all_labels_original.append(dataloader.dataset.labels_original[[d.num_nodes for d in data.to('cpu').to_data_list()]])\n",
        "\n",
        "    # é€†å¤‰æ›ã—ãŸäºˆæ¸¬å€¤ã§MAEã‚’è¨ˆç®—\n",
        "    all_preds_inversed = np.concatenate(all_preds_inversed)\n",
        "    all_labels_original = np.concatenate(all_labels_original)\n",
        "\n",
        "    final_scores = {}\n",
        "    for i, target in enumerate(CFG.targets):\n",
        "        mask = ~np.isnan(all_labels_original[:, i])\n",
        "        final_scores[target] = mean_absolute_error(all_labels_original[mask, i], all_preds_inversed[mask, i])\n",
        "\n",
        "    return total_loss / len(dataloader), final_scores"
      ],
      "metadata": {
        "id": "cKTmecYcoKgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df = load_data(CFG) # è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å…¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿\n",
        "\n",
        "kf = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
        "oof_preds = pd.DataFrame(index=train_val_df.index, columns=CFG.targets)\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_df)):\n",
        "    print(f\"========== Fold {fold+1}/{CFG.n_splits} ==========\")\n",
        "    train_fold_df = train_val_df.iloc[train_idx]\n",
        "    valid_fold_df = train_val_df.iloc[val_idx]\n",
        "\n",
        "    # --- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ä½œæˆãƒ»ä¿å­˜ ---\n",
        "    target_scalers = {}\n",
        "    for target in CFG.targets:\n",
        "        scaler = StandardScaler()\n",
        "        valid_targets = train_fold_df[target].dropna().values.reshape(-1, 1)\n",
        "        scaler.fit(valid_targets)\n",
        "        target_scalers[target] = scaler\n",
        "\n",
        "    scaler_path = os.path.join(CFG.output_dir, f\"gnn_target_scalers_fold_{fold}.joblib\")\n",
        "    joblib.dump(target_scalers, scaler_path)\n",
        "    print(f\"Target scalers for fold {fold} saved to {scaler_path}\")\n",
        "\n",
        "    # --- Datasetã¨DataLoaderã®æº–å‚™ ---\n",
        "    train_dataset = PolymerDataset(train_fold_df, target_scalers)\n",
        "    valid_dataset = PolymerDataset(valid_fold_df, target_scalers)\n",
        "    train_loader = PyGDataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
        "    valid_loader = PyGDataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "\n",
        "    # --- ãƒ¢ãƒ‡ãƒ«ã€æå¤±é–¢æ•°ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å®šç¾© ---\n",
        "    model = PolymerGNN().to(CFG.device)\n",
        "    criterion = MaskedMAELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
        "\n",
        "    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ— ---\n",
        "    best_val_score = float('inf')\n",
        "    patience_counter = 0\n",
        "    model_path = os.path.join(CFG.output_dir, f\"best_gnn_model_fold_{fold}.pth\")\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        train_loss = train_fn(model, train_loader, optimizer, criterion, CFG.device)\n",
        "        val_loss, val_scores = eval_fn(model, valid_loader, criterion, CFG.device, target_scalers)\n",
        "\n",
        "        avg_val_score = np.mean(list(val_scores.values()))\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid MAE: {avg_val_score:.4f}\")\n",
        "\n",
        "        if avg_val_score < best_val_score:\n",
        "            best_val_score = avg_val_score\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f\"â­ï¸ Best model saved with score: {best_val_score:.4f}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= CFG.patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"GNNãƒ¢ãƒ‡ãƒ«ã®CVå­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ğŸ‰\")"
      ],
      "metadata": {
        "id": "Vu069Neqpy3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# OOFäºˆæ¸¬ã¨ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®ä¿å­˜\n",
        "# ===================================================================\n",
        "\n",
        "# OOFäºˆæ¸¬ã®ä¿å­˜\n",
        "oof_preds_path = os.path.join(CFG.output_dir, \"gnn_oof_predictions.csv\")\n",
        "oof_preds.to_csv(oof_preds_path, index=False)\n",
        "print(f\"âœ… OOF predictions saved to: {oof_preds_path}\")\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®ä¿å­˜ (Placeholder - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨äºˆæ¸¬ã‚³ãƒ¼ãƒ‰ã¯åˆ¥é€”å¿…è¦)\n",
        "test_preds_path = os.path.join(CFG.output_dir, \"gnn_test_predictions.csv\")\n",
        "test_preds_df.to_csv(test_preds_path, index=False)\n",
        "print(f\"âœ… Test predictions saved to: {test_preds_path}\")"
      ],
      "metadata": {
        "id": "paVOqcl74hcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# (CVå­¦ç¿’ãƒ«ãƒ¼ãƒ—ãŒå®Œäº†ã—ãŸå¾Œ...)\n",
        "\n",
        "print(\"Calculating final OOF CV scores...\")\n",
        "\n",
        "# æœ€çµ‚çš„ãªCVã‚¹ã‚³ã‚¢ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
        "final_cv_scores = {}\n",
        "\n",
        "# å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®OOFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "for target in CFG.targets:\n",
        "    # æ­£è§£ãƒ©ãƒ™ãƒ«ã¨OOFäºˆæ¸¬ã‹ã‚‰NaNã‚’é™¤å¤–ã—ã¦æ•´åˆæ€§ã‚’å–ã‚‹\n",
        "    y_true = train_val_df[target].dropna()\n",
        "    y_pred = oof_preds[target].loc[y_true.index].dropna()\n",
        "\n",
        "    score = mean_absolute_error(y_true, y_pred)\n",
        "    final_cv_scores[target] = score\n",
        "    print(f\"Final OOF Score for {target}: {score:.5f}\")\n",
        "\n",
        "# å…¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
        "average_score = np.mean(list(final_cv_scores.values()))\n",
        "final_cv_scores['average_cv_score'] = average_score\n",
        "print(f\"Average OOF CV Score: {average_score:.5f}\")\n",
        "\n",
        "# ã‚¹ã‚³ã‚¢ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
        "scores_path = os.path.join(CFG.output_dir, \"gnn_cv_scores.json\")\n",
        "with open(scores_path, 'w') as f:\n",
        "    json.dump(final_cv_scores, f, indent=4)\n",
        "\n",
        "print(f\"âœ… Final CV scores saved to: {scores_path}\")"
      ],
      "metadata": {
        "id": "dPY8rIR82KaL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
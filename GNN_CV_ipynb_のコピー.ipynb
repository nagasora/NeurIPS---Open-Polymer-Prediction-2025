{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP70uX1rSTkom15USVkAaLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagasora/NeurIPS---Open-Polymer-Prediction-2025/blob/main/GNN_CV_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwOAv4XnDtG"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "neurips_open_polymer_prediction_2025_path = kagglehub.competition_download('neurips-open-polymer-prediction-2025')\n",
        "senkin13_rdkit_2025_3_3_cp311_path = kagglehub.dataset_download('senkin13/rdkit-2025-3-3-cp311')\n",
        "minatoyukinaxlisa_tc_smiles_path = kagglehub.dataset_download('minatoyukinaxlisa/tc-smiles')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "iJsnFnwOnGa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'neurips_open_polymer_prediction_2025_path: {neurips_open_polymer_prediction_2025_path}')"
      ],
      "metadata": {
        "id": "RYMTrCf8nIP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 1. ライブラリのインストール\n",
        "# ===================================================================\n",
        "!pip install rdkit --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install torch_geometric --quiet"
      ],
      "metadata": {
        "id": "1UDXoKR6nJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2. 必要なライブラリのインポート\n",
        "# ===================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Tuple\n",
        "import warnings\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F # Import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "TMxAA7uZnOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 3. 設定クラスと基本設定\n",
        "# ===================================================================\n",
        "class CFG:\n",
        "    seed = 42\n",
        "    n_splits = 5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    data_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/'\n",
        "    supp_dir = '/root/.cache/kagglehub/competitions/neurips-open-polymer-prediction-2025/train_supplement/'\n",
        "    output_dir = '/content/drive/MyDrive/kaggle notebook/NeurIPS - Open Polymer Prediction 2025/LGBM+XGB+GNN_CV/'\n",
        "    lgbm_model_dir = os.path.join(output_dir, 'models_lgbm')\n",
        "    xgb_model_dir = os.path.join(output_dir, 'models_xgb')\n",
        "    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
        "\n",
        "    # Model Hyperparameters\n",
        "    node_feature_dim = 40\n",
        "    hidden_dim = 256\n",
        "    output_dim = 5\n",
        "    num_gnn_layers = 4\n",
        "    dropout_rate = 0.2\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    epochs = 100 # エポック数を増やす\n",
        "    batch_size = 64\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 1e-5\n",
        "    patience = 10 # Early Stoppingのためのpatience\n",
        "\n",
        "    targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "seed_everything(CFG.seed)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "em8LfuefnRhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_IGYGjhn7wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(cfg: CFG) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    メインの学習データと追加データを読み込み、統合する関数。\n",
        "\n",
        "    Args:\n",
        "        cfg (CFG): データパスを含む設定オブジェクト\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: 全ての学習データを結合したデータフレーム\n",
        "    \"\"\"\n",
        "    print(\"データの読み込みを開始します...\")\n",
        "\n",
        "    # 1. メインの学習データを読み込み\n",
        "    main_train_path = os.path.join(cfg.data_dir, 'train.csv')\n",
        "    try:\n",
        "        train_df = pd.read_csv(main_train_path)\n",
        "        print(f\"メイン学習データを読み込みました。サンプル数: {len(train_df)}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"メイン学習データが見つかりません: {main_train_path}\")\n",
        "        # メインデータがない場合は空のDataFrameを返して終了\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. 追加データの読み込みと整形\n",
        "    try:\n",
        "        # dataset1: Tc (熱伝導率) データ\n",
        "        supp1_path = os.path.join(cfg.supp_dir, 'dataset1.csv')\n",
        "        supp1 = pd.read_csv(supp1_path)[['SMILES', 'TC_mean']].rename(columns={'TC_mean': 'Tc'})\n",
        "\n",
        "        # dataset3: Tg (ガラス転移温度) データ\n",
        "        supp3_path = os.path.join(cfg.supp_dir, 'dataset3.csv')\n",
        "        supp3 = pd.read_csv(supp3_path)[['SMILES', 'Tg']]\n",
        "\n",
        "        # dataset4: FFV (自由体積分率) データ\n",
        "        supp4_path = os.path.join(cfg.supp_dir, 'dataset4.csv')\n",
        "        supp4 = pd.read_csv(supp4_path)[['SMILES', 'FFV']]\n",
        "\n",
        "        # 3. 全てのデータフレームを結合\n",
        "        train_df = pd.concat([train_df, supp1, supp3, supp4], ignore_index=True)\n",
        "\n",
        "        # 重複するSMILESがあれば削除 (任意ですが、クリーニングとして推奨)\n",
        "        train_df = train_df.drop_duplicates(subset=['SMILES']).reset_index(drop=True)\n",
        "\n",
        "        print(f\"✅ 追加データを統合しました。総学習サンプル数: {len(train_df)}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"⚠️ 追加データが見つかりませんでした: {e}\")\n",
        "        print(\"メインデータのみで学習を続行します。\")\n",
        "\n",
        "    return train_df"
      ],
      "metadata": {
        "id": "Tkiz9cvmnby0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# SMILESからグラフへの変換\n",
        "# ===================================================================\n",
        "def smiles_to_graph(smiles: str) -> Data:\n",
        "    \"\"\"\n",
        "    SMILES文字列をPyTorch GeometricのDataオブジェクトに変換する。\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILES文字列\n",
        "\n",
        "    Returns:\n",
        "        Data: ノード特徴量、エッジインデックスを含むグラフオブジェクト\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "\n",
        "    # ノード（原子）特徴量の抽出\n",
        "    atom_features = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        feature = [\n",
        "            atom.GetAtomicNum(),\n",
        "            atom.GetDegree(),\n",
        "            atom.GetFormalCharge(),\n",
        "            atom.GetNumRadicalElectrons(),\n",
        "            atom.GetHybridization(),\n",
        "            atom.GetIsAromatic(),\n",
        "            atom.GetTotalNumHs(),\n",
        "        ]\n",
        "        atom_features.append(feature)\n",
        "    x = torch.tensor(atom_features, dtype=torch.float)\n",
        "\n",
        "    # エッジ（結合）情報の抽出\n",
        "    edge_indices = []\n",
        "    for bond in mol.GetBonds():\n",
        "        i = bond.GetBeginAtomIdx()\n",
        "        j = bond.GetEndAtomIdx()\n",
        "        edge_indices.extend([[i, j], [j, i]]) # 無向グラフ\n",
        "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # 特徴量ベクトルの次元を合わせるためのパディング\n",
        "    if x.shape[1] < CFG.node_feature_dim:\n",
        "        padding = torch.zeros(x.shape[0], CFG.node_feature_dim - x.shape[1])\n",
        "        x = torch.cat([x, padding], dim=1)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# GNNモデルの定義\n",
        "# ===================================================================\n",
        "class PolymerGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ポリマー物性予測のためのGNNモデル\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(PolymerGNN, self).__init__()\n",
        "        # GIN畳み込み層\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(GINConv(nn.Sequential(\n",
        "            nn.Linear(CFG.node_feature_dim, CFG.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "        )))\n",
        "        self.batch_norms.append(nn.BatchNorm1d(CFG.hidden_dim))\n",
        "\n",
        "        for _ in range(CFG.num_gnn_layers - 1):\n",
        "            self.convs.append(GINConv(nn.Sequential(\n",
        "                nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(CFG.hidden_dim, CFG.hidden_dim),\n",
        "            )))\n",
        "            self.batch_norms.append(nn.BatchNorm1d(CFG.hidden_dim))\n",
        "\n",
        "        # 出力層（MLP）\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(CFG.hidden_dim, CFG.hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(CFG.dropout_rate),\n",
        "            nn.Linear(CFG.hidden_dim // 2, CFG.output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, data: Data) -> torch.Tensor:\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.batch_norms):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "\n",
        "        # グラフ全体の表現ベクトルを計算\n",
        "        x_pooled = global_add_pool(x, batch)\n",
        "\n",
        "        # MLPで最終的な出力を計算\n",
        "        out = self.mlp(x_pooled)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ===================================================================\n",
        "# 4. Datasetと損失関数の再定義（スケーリング対応）\n",
        "# ===================================================================\n",
        "class PolymerDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, target_scalers: Dict[str, StandardScaler] = None):\n",
        "        self.smiles = df['SMILES'].values\n",
        "        self.labels_original = df[CFG.targets].values\n",
        "        self.target_scalers = target_scalers\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple:\n",
        "        graph = smiles_to_graph(self.smiles[idx])\n",
        "        labels = self.labels_original[idx]\n",
        "\n",
        "        if self.target_scalers:\n",
        "            # ▼▼▼ 重要な変更点：ラベルをスケーリング ▼▼▼\n",
        "            scaled_labels = np.full_like(labels, np.nan, dtype=np.float32)\n",
        "            for i, target in enumerate(CFG.targets):\n",
        "                if not np.isnan(labels[i]):\n",
        "                    scaled_labels[i] = self.target_scalers[target].transform(labels[i].reshape(1, 1))[0, 0]\n",
        "            return graph, torch.tensor(scaled_labels, dtype=torch.float)\n",
        "        else:\n",
        "            return graph, torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "# ===================================================================\n",
        "# 損失関数と学習・評価ループ\n",
        "# ===================================================================\n",
        "class MaskedMAELoss(nn.Module):\n",
        "    \"\"\"NaNを無視してMAEを計算するカスタム損失関数\"\"\"\n",
        "    def __init__(self):\n",
        "        super(MaskedMAELoss, self).__init__()\n",
        "        self.mae = nn.L1Loss(reduction='none')\n",
        "\n",
        "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "        mask = ~torch.isnan(y_true)\n",
        "        loss = self.mae(y_pred[mask], y_true[mask])\n",
        "        return loss.mean()\n",
        "\n",
        "def train_fn(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"1エポック分の学習を行う関数\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def eval_fn(model, dataloader, criterion, device, target_scalers):\n",
        "    \"\"\"評価関数をスケーリング対応に修正\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds_inversed = []\n",
        "    all_labels_original = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, scaled_labels in dataloader:\n",
        "            data, scaled_labels = data.to(device), scaled_labels.to(device)\n",
        "\n",
        "            # モデルはスケーリングされた値を予測\n",
        "            scaled_preds = model(data)\n",
        "            loss = criterion(scaled_preds, scaled_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # ▼▼▼ 重要な変更点：予測値を元のスケールに戻す ▼▼▼\n",
        "            scaled_preds_cpu = scaled_preds.cpu().numpy()\n",
        "            inversed_preds = np.full_like(scaled_preds_cpu, np.nan)\n",
        "\n",
        "            for i, target in enumerate(CFG.targets):\n",
        "                # NaNでない予測値のみを逆変換\n",
        "                valid_mask = ~np.isnan(scaled_preds_cpu[:, i])\n",
        "                if np.any(valid_mask):\n",
        "                    inversed_preds[valid_mask, i] = target_scalers[target].inverse_transform(scaled_preds_cpu[valid_mask, i].reshape(-1, 1)).flatten()\n",
        "\n",
        "            all_preds_inversed.append(inversed_preds)\n",
        "            all_labels_original.append(dataloader.dataset.labels_original[[d.num_nodes for d in data.to('cpu').to_data_list()]])\n",
        "\n",
        "    # 逆変換した予測値でMAEを計算\n",
        "    all_preds_inversed = np.concatenate(all_preds_inversed)\n",
        "    all_labels_original = np.concatenate(all_labels_original)\n",
        "\n",
        "    final_scores = {}\n",
        "    for i, target in enumerate(CFG.targets):\n",
        "        mask = ~np.isnan(all_labels_original[:, i])\n",
        "        final_scores[target] = mean_absolute_error(all_labels_original[mask, i], all_preds_inversed[mask, i])\n",
        "\n",
        "    return total_loss / len(dataloader), final_scores"
      ],
      "metadata": {
        "id": "cKTmecYcoKgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df = load_data(CFG) # 追加データを含む全学習データ\n",
        "\n",
        "kf = KFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
        "oof_preds = pd.DataFrame(index=train_val_df.index, columns=CFG.targets)\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_df)):\n",
        "    print(f\"========== Fold {fold+1}/{CFG.n_splits} ==========\")\n",
        "    train_fold_df = train_val_df.iloc[train_idx]\n",
        "    valid_fold_df = train_val_df.iloc[val_idx]\n",
        "\n",
        "    # --- ターゲットのスケーラーを学習データで作成・保存 ---\n",
        "    target_scalers = {}\n",
        "    for target in CFG.targets:\n",
        "        scaler = StandardScaler()\n",
        "        valid_targets = train_fold_df[target].dropna().values.reshape(-1, 1)\n",
        "        scaler.fit(valid_targets)\n",
        "        target_scalers[target] = scaler\n",
        "\n",
        "    scaler_path = os.path.join(CFG.output_dir, f\"gnn_target_scalers_fold_{fold}.joblib\")\n",
        "    joblib.dump(target_scalers, scaler_path)\n",
        "    print(f\"Target scalers for fold {fold} saved to {scaler_path}\")\n",
        "\n",
        "    # --- DatasetとDataLoaderの準備 ---\n",
        "    train_dataset = PolymerDataset(train_fold_df, target_scalers)\n",
        "    valid_dataset = PolymerDataset(valid_fold_df, target_scalers)\n",
        "    train_loader = PyGDataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
        "    valid_loader = PyGDataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "\n",
        "    # --- モデル、損失関数、オプティマイザの定義 ---\n",
        "    model = PolymerGNN().to(CFG.device)\n",
        "    criterion = MaskedMAELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
        "\n",
        "    # --- 学習ループ ---\n",
        "    best_val_score = float('inf')\n",
        "    patience_counter = 0\n",
        "    model_path = os.path.join(CFG.output_dir, f\"best_gnn_model_fold_{fold}.pth\")\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        train_loss = train_fn(model, train_loader, optimizer, criterion, CFG.device)\n",
        "        val_loss, val_scores = eval_fn(model, valid_loader, criterion, CFG.device, target_scalers)\n",
        "\n",
        "        avg_val_score = np.mean(list(val_scores.values()))\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid MAE: {avg_val_score:.4f}\")\n",
        "\n",
        "        if avg_val_score < best_val_score:\n",
        "            best_val_score = avg_val_score\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f\"⭐️ Best model saved with score: {best_val_score:.4f}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= CFG.patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"GNNモデルのCV学習が完了しました。🎉\")"
      ],
      "metadata": {
        "id": "Vu069Neqpy3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# OOF予測とテスト予測の保存\n",
        "# ===================================================================\n",
        "\n",
        "# OOF予測の保存\n",
        "oof_preds_path = os.path.join(CFG.output_dir, \"gnn_oof_predictions.csv\")\n",
        "oof_preds.to_csv(oof_preds_path, index=False)\n",
        "print(f\"✅ OOF predictions saved to: {oof_preds_path}\")\n",
        "\n",
        "# テスト予測の保存 (Placeholder - テストデータ読み込みと予測コードは別途必要)\n",
        "test_preds_path = os.path.join(CFG.output_dir, \"gnn_test_predictions.csv\")\n",
        "test_preds_df.to_csv(test_preds_path, index=False)\n",
        "print(f\"✅ Test predictions saved to: {test_preds_path}\")"
      ],
      "metadata": {
        "id": "paVOqcl74hcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# (CV学習ループが完了した後...)\n",
        "\n",
        "print(\"Calculating final OOF CV scores...\")\n",
        "\n",
        "# 最終的なCVスコアを格納する辞書\n",
        "final_cv_scores = {}\n",
        "\n",
        "# 各ターゲットのOOFスコアを計算\n",
        "for target in CFG.targets:\n",
        "    # 正解ラベルとOOF予測からNaNを除外して整合性を取る\n",
        "    y_true = train_val_df[target].dropna()\n",
        "    y_pred = oof_preds[target].loc[y_true.index].dropna()\n",
        "\n",
        "    score = mean_absolute_error(y_true, y_pred)\n",
        "    final_cv_scores[target] = score\n",
        "    print(f\"Final OOF Score for {target}: {score:.5f}\")\n",
        "\n",
        "# 全ターゲットの平均スコアを計算\n",
        "average_score = np.mean(list(final_cv_scores.values()))\n",
        "final_cv_scores['average_cv_score'] = average_score\n",
        "print(f\"Average OOF CV Score: {average_score:.5f}\")\n",
        "\n",
        "# スコアをJSONファイルに保存\n",
        "scores_path = os.path.join(CFG.output_dir, \"gnn_cv_scores.json\")\n",
        "with open(scores_path, 'w') as f:\n",
        "    json.dump(final_cv_scores, f, indent=4)\n",
        "\n",
        "print(f\"✅ Final CV scores saved to: {scores_path}\")"
      ],
      "metadata": {
        "id": "dPY8rIR82KaL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}